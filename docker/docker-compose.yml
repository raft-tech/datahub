# Docker compose file covering DataHub's default configuration, which is to run all containers on a single host.

# Please see the README.md for instructions as to how to use and customize.

# NOTE: This file will cannot build! No dockerfiles are set. See the README.md in this directory.
---
version: '3.8'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:5.4.0
    env_file: zookeeper/env/docker.env
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    volumes:
      - zkdata:/var/opt/zookeeper

  broker:
    image: confluentinc/cp-kafka:5.4.0
    env_file: broker/env/docker.env
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
      - "9092:9092"

  # This "container" is a workaround to pre-create topics
  kafka-setup:
    build:
      context: kafka-setup
    image: linkedin/datahub-kafka-setup:${DATAHUB_VERSION:-head}
    env_file: kafka-setup/env/docker.env
    hostname: kafka-setup
    container_name: kafka-setup
    depends_on:
      - broker
      - schema-registry

  schema-registry:
    image: confluentinc/cp-schema-registry:5.4.0
    env_file: schema-registry/env/docker.env
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - zookeeper
      - broker
    ports:
      - "8081:8081"

  elasticsearch:
    image: elasticsearch:7.9.3
    env_file: elasticsearch/env/docker.env
    container_name: elasticsearch
    hostname: elasticsearch
    ports:
      - "9200:9200"
    volumes:
      - esdata:/usr/share/elasticsearch/data
    healthcheck:
        test: ["CMD-SHELL", "curl -sS --fail 'http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=0s' || exit 1"]
        start_period: 2m
        retries: 4

  neo4j:
    image: neo4j:4.0.6
    env_file: neo4j/env/docker.env
    hostname: neo4j
    container_name: neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    volumes:
      - neo4jdata:/data

  # This "container" is a workaround to pre-create search indices
  elasticsearch-setup:
    build:
      context: ../
      dockerfile: docker/elasticsearch-setup/Dockerfile
    image: linkedin/datahub-elasticsearch-setup:${DATAHUB_VERSION:-head}
    env_file: elasticsearch-setup/env/docker.env
    hostname: elasticsearch-setup
    container_name: elasticsearch-setup
    depends_on:
      - elasticsearch

  datahub-gms:
    build:
        context: ../
        dockerfile: docker/datahub-gms/Dockerfile
    image: linkedin/datahub-gms:${DATAHUB_VERSION:-head}
    hostname: datahub-gms
    container_name: datahub-gms
    environment:
      - DATASET_ENABLE_SCSI=false
      - EBEAN_DATASOURCE_USERNAME=datahub
      - EBEAN_DATASOURCE_PASSWORD=datahub
      - EBEAN_DATASOURCE_HOST=mysql:3306
      - EBEAN_DATASOURCE_URL=jdbc:mysql://mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8
      - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - KAFKA_SCHEMAREGISTRY_URL=http://schema-registry:8081
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - GRAPH_SERVICE_IMPL=elasticsearch
      - JAVA_OPTS=-Xms1g -Xmx1g
      - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
      - MAE_CONSUMER_ENABLED=true
      - MCE_CONSUMER_ENABLED=true
    ports:
      - "8080:8080"
    depends_on:
      - elasticsearch-setup
      - kafka-setup
      - mysql
      - neo4j

  datahub-frontend-react:
    build:
      context: ../
      dockerfile: docker/datahub-frontend/Dockerfile
    image: linkedin/datahub-frontend-react:${DATAHUB_VERSION:-head}
    env_file: datahub-frontend/env/docker.env
    environment:
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - DATAHUB_SECRET=YouKnowNothing
      - DATAHUB_APP_VERSION=1.0
      - DATAHUB_PLAY_MEM_BUFFER_SIZE=10MB
      - JAVA_OPTS=-Xms512m -Xmx512m -Dhttp.port=9002 -Dconfig.file=datahub-frontend/conf/application.conf
        -Djava.security.auth.login.config=datahub-frontend/conf/jaas.conf -Dlogback.configurationFile=datahub-frontend/conf/logback.xml
        -Dlogback.debug=false -Dpidfile.path=/dev/null
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - DATAHUB_TRACKING_TOPIC=DataHubUsageEvent_v1
      - ELASTIC_CLIENT_HOST=elasticsearch
      - ELASTIC_CLIENT_PORT=9200
      - AUTH_OIDC_ENABLED=true
      - AUTH_OIDC_CLIENT_ID=data-fabric-confidential
      - AUTH_OIDC_CLIENT_SECRET=e96ef414-de3e-460c-bd91-0866debfd8eb
      - AUTH_OIDC_DISCOVERY_URI=http://${IP_ADDRESS-localhost}:8100/auth/realms/data-fabric-realm/.well-known/openid-configuration
      - AUTH_OIDC_BASE_URL=http://${IP_ADDRESS-localhost}:9002
      - AUTH_JAAS_ENABLED=false
      - AUTH_OIDC_JIT_PROVISIONING_ENABLED=true
      - AUTH_OIDC_PRE_PROVISIONING_REQUIRED=false
      - AUTH_OIDC_EXTRACT_GROUPS_ENABLED=true
      - AUTH_OIDC_GROUPS_CLAIM=groups
    hostname: datahub-frontend-react
    container_name: datahub-frontend-react
    ports:
      - "9002:9002"
    depends_on:
      - datahub-gms

networks:
  default:
    name: datahub_network

volumes:
  esdata:
  neo4jdata:
  zkdata:
